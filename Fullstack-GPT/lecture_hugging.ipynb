{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-22T08:19:19.710386Z",
     "start_time": "2024-07-22T08:19:11.970858Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "\"\"\" Todo: 강의랑 달라진 점\n",
    "Langchain과 HuggingFace의 버전 업데이트에 따라 더이상 Langchaing에서 제공하는 HuggingFaceHub 내부 의존성의 InterfaceAPI를 사용하지 않아. \n",
    "그래서 HuggingFaceEndpoint를 사용하도록 변경했다.\n",
    "\n",
    "Langchaing과 HuggingFace-hub의 버전을 최신화하였다.\n",
    "\"\"\"\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\"[INST]What is te meaning of {word}? \\n Please Answer to KOREAN.[/INST]\")\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#     model_kwargs={\n",
    "#         \"max_new_tokens\": 250,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#     max_new_tokens=250,\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a.\")\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='gpt2',\n",
    "    task='text-generation',\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 50\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"word\": \"tomato\"\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyj/Documents/Projects/Python-Servers/Fullstack-GPT/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A tomato is a. That's where it comes from. It grew on grapes for a few centuries, and over time they had to evolve, like most plants have.\\n\\nMolecular studies have been done on an entire race of tomato plants to detect genetic signatures of\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-30T06:59:43.259049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "\"\"\" Todo: 강의랑 달라진 점\n",
    "GPT4ALL의 사용 방법이 강의랑 달라졌다. 이제 Window, Mac, Bin으로 어플리케이션을 받아야 Local 모델을 다운하든 할 수 있다. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a.\")\n",
    "llm = GPT4All(\n",
    "    model=\"/Users/cyj/Library/Application Support/nomic.ai/GPT4All/gpt4all-falcon-newbpe-q4_0.gguf\",\n",
    "    callbacks=[],\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"word\": \"tomato\"\n",
    "})"
   ],
   "id": "d8ddc7c338a73b89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:02:59.657837Z",
     "start_time": "2024-07-30T08:02:58.551330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def get_weather(lon, lat):\n",
    "    print(f\"call an api...\")\n",
    "    \n",
    "function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"function that takes longitude and latitude to find the weather of a place\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"lon\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"the longitude coordinate\"\n",
    "            },\n",
    "            \"lat\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"the latitude coordinate\"\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"lon\", \"lat\"],\n",
    "}\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ").bind(\n",
    "    function_call=\"auto\",\n",
    "    functions=[\n",
    "        function\n",
    "    ]\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\"Who is the weather in {city}\")\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\n",
    "    \"city\": \"Seoul\",\n",
    "})\n",
    "\n",
    "print(response)\n",
    "print(response.additional_kwargs[\"function_call\"][\"arguments\"])\n",
    "\n",
    "import json\n",
    "r = json.loads(response)\n",
    "get_weather(r['lon'], r['lat'])"
   ],
   "id": "a57953e66d3fe453",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'arguments': '{\"lon\":\"126.9779\",\"lat\":\"37.5665\"}', 'name': 'get_weather'}} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 73, 'total_tokens': 97}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run-5aa6d516-1d5f-419c-9bce-2284d3efc1ea-0'\n",
      "{\"lon\":\"126.9779\",\"lat\":\"37.5665\"}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not AIMessage",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 44\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(response\u001B[38;5;241m.\u001B[39madditional_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marguments\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m---> 44\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m get_weather(r[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlon\u001B[39m\u001B[38;5;124m'\u001B[39m], r[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlat\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/json/__init__.py:339\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    338\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(s, (\u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mbytearray\u001B[39m)):\n\u001B[0;32m--> 339\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthe JSON object must be str, bytes or bytearray, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    340\u001B[0m                         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n",
      "\u001B[0;31mTypeError\u001B[0m: the JSON object must be str, bytes or bytearray, not AIMessage"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:23:15.455434Z",
     "start_time": "2024-07-30T08:23:13.137531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "function = {\n",
    "    \"name\": \"create_quiz\",\n",
    "    \"description\": \"function that takes a list of questions and answers and returns a quiz\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"questions\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                        },\n",
    "                        \"answers\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"answer\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                    },\n",
    "                                    \"correct\": {\n",
    "                                        \"type\": \"boolean\",\n",
    "                                    },\n",
    "                                },\n",
    "                                \"required\": [\"answer\", \"correct\"],\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"question\", \"answers\"],\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"questions\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ").bind(\n",
    "    function_call={\n",
    "        \"name\": \"create_quiz\",\n",
    "    },\n",
    "    functions=[\n",
    "        function,\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Make a quiz about {city}\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"city\": \"rome\"})\n",
    "\n",
    "\n",
    "response = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "\n",
    "print(response)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "for question in json.loads(response)[\"questions\"]:\n",
    "    print(question)"
   ],
   "id": "d4a5f66f2a696d1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"questions\":[{\"question\":\"What year was Rome founded?\",\"answers\":[{\"answer\":\"753 BC\",\"correct\":true},{\"answer\":\"500 AD\",\"correct\":false},{\"answer\":\"100 BC\",\"correct\":false}]},{\"question\":\"Who was the first emperor of Rome?\",\"answers\":[{\"answer\":\"Julius Caesar\",\"correct\":false},{\"answer\":\"Augustus\",\"correct\":true},{\"answer\":\"Nero\",\"correct\":false}]},{\"question\":\"What famous structure in Rome was built by the ancient Romans?\",\"answers\":[{\"answer\":\"Eiffel Tower\",\"correct\":false},{\"answer\":\"Colosseum\",\"correct\":true},{\"answer\":\"Big Ben\",\"correct\":false}]}]}\n",
      "{'question': 'What year was Rome founded?', 'answers': [{'answer': '753 BC', 'correct': True}, {'answer': '500 AD', 'correct': False}, {'answer': '100 BC', 'correct': False}]}\n",
      "{'question': 'Who was the first emperor of Rome?', 'answers': [{'answer': 'Julius Caesar', 'correct': False}, {'answer': 'Augustus', 'correct': True}, {'answer': 'Nero', 'correct': False}]}\n",
      "{'question': 'What famous structure in Rome was built by the ancient Romans?', 'answers': [{'answer': 'Eiffel Tower', 'correct': False}, {'answer': 'Colosseum', 'correct': True}, {'answer': 'Big Ben', 'correct': False}]}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "64fd9df4c1617e9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
