{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-22T08:19:19.710386Z",
     "start_time": "2024-07-22T08:19:11.970858Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "\"\"\" Todo: 강의랑 달라진 점\n",
    "Langchain과 HuggingFace의 버전 업데이트에 따라 더이상 Langchaing에서 제공하는 HuggingFaceHub 내부 의존성의 InterfaceAPI를 사용하지 않아. \n",
    "그래서 HuggingFaceEndpoint를 사용하도록 변경했다.\n",
    "\n",
    "Langchaing과 HuggingFace-hub의 버전을 최신화하였다.\n",
    "\"\"\"\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\"[INST]What is te meaning of {word}? \\n Please Answer to KOREAN.[/INST]\")\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#     model_kwargs={\n",
    "#         \"max_new_tokens\": 250,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#     max_new_tokens=250,\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a.\")\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='gpt2',\n",
    "    task='text-generation',\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 50\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"word\": \"tomato\"\n",
    "})"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyj/Documents/Projects/Python-Servers/Fullstack-GPT/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A tomato is a. That's where it comes from. It grew on grapes for a few centuries, and over time they had to evolve, like most plants have.\\n\\nMolecular studies have been done on an entire race of tomato plants to detect genetic signatures of\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-22T08:19:33.857906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "\"\"\" Todo: 강의랑 달라진 점\n",
    "GPT4ALL의 사용 방법이 강의랑 달라졌다. 이제 Window, Mac, Bin으로 어플리케이션을 받아야 Local 모델을 다운하든 할 수 있다. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a.\")\n",
    "llm = GPT4All(\n",
    "    model=\"/Users/cyj/Library/Application Support/nomic.ai/GPT4All/gpt4all-falcon-newbpe-q4_0.gguf\",\n",
    "    callbacks=[],\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"word\": \"tomato\"\n",
    "})"
   ],
   "id": "d8ddc7c338a73b89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a57953e66d3fe453"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
